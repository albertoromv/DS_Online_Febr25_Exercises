{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/cabecera.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a ver cómo funciona un word embedding de una forma simulada al tiempo que vemos como emplear la capa de embeddings de Keras, luego veremos como hacer sentence embedding utilizando un modulo o modelo preentrenado de Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un word embedding transforma las palabras de un texto en un vector de n dimensiones. Veamos como hacerlo con una capa de embeddings, sin entrenar y así podrás ver como instanciarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias_ejemplo = [\"Me\",\"llamo\",\"Iñigo\",\"Montoya\",\"soy\",\"tú\",\"mataste\",\"a\",\"mi\",\"padre\"] # Una frase o un vocabulario de ejemplo\n",
    "pre_conversion = tf.keras.layers.StringLookup() # IMPORTANTE: Hay que convertir nuestro vocabulario a indices \n",
    "pre_conversion.adapt(categorias_ejemplo) # Y como ya habíamos visto, hay que hacer un fit/adapt\n",
    "\n",
    "lookup_y_embedding = tf.keras.Sequential([tf.keras.layers.InputLayer(shape=[], dtype=tf.string), \n",
    "                                          pre_conversion,\n",
    "                                          tf.keras.layers.Embedding(input_dim = pre_conversion.vocabulary_size(),\n",
    "                                                                    output_dim = 2)])\n",
    "# input_dim -> Tamaño del vocabulario a convertir en vectores de output_dim dimensiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este \"modelo\" no resuelve ningún tipo de problema solo pasa las palabras a traves de la capa de codificación y luego de la embeddings y genera por cadda palabra un vector de 2 dimensiones (output_dim). Pero además como no está entrenada funcionará porque tiene pesos inicializados de forma aleatoria. Es decir que si le pasamos como entrada la variable con la frase de ejemplo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_conversion(categorias_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_y_embedding(np.array(categorias_ejemplo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos convierte cada palabra en un embedding sin sentido. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de hacerlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frase = \"Me llamo Iñigo Montoya\"\n",
    "lookup_y_embedding(np.array(frase.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder darle valor tendríamos que incluir nuestras dos capas (la codificadora y la de embedding) en un modelo con un objetivo determinado y la capa de embeddings se entrenaría para generar los embeddings que mejor se adapten al problema a solucionar con ese modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a convertir una serie de frases en embeddings. En concreto de 50 dimensiones. Lo haremos utilizando un modelo preentrenado el nnlem-en-dim50 de Google. Internamente es un modelo word embeddings que convierte cada palabra en un embedding de 50 dimensiones y luego calcular el centroide de todos los vectores obtenidos para una frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\") # Así se obtiene el modelo\n",
    "sentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n",
    "sentence_embeddings.numpy().round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora algunas cosas como por ejemplo obtener la similitud entre sentencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['El Real Madrid lo tiene difícil para ganar al Manchester City.',\n",
    " 'El Barcelona puede clasificar frente al PSG, si se esfuerza.',\n",
    " 'Las tropas rusas han tomado Dubrovnik.',\n",
    " 'El ejercito ucraniano se ha replegado']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = hub_layer(tf.constant(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "\n",
    "for (frase1,vec1),(frase2,vec2) in combinations(zip(sentences,sentence_embeddings.numpy()), r = 2):\n",
    "    print(frase1,\"vs\",frase2, cosine_similarity([vec1],[vec2]), np.linalg.norm(vec1-vec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el coseno tendríamos algún problema con la distancia quedan mejor emparejadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"¿Contra quién juega el Barcelona?\"\n",
    "pregunta = hub_layer(tf.constant([question]))\n",
    "vec_q = pregunta.numpy()\n",
    "distancias = []\n",
    "respuestas = []\n",
    "\n",
    "for answer,vec_a in zip(sentences,sentence_embeddings.numpy()):\n",
    "    respuestas.append(answer)\n",
    "    distancias.append(np.linalg.norm(vec_q-vec_a))\n",
    "    \n",
    "print(f\"P:{question}\")\n",
    "print(f\"R:{respuestas[np.argmin(distancias)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"¿Qué hacen los ucranianos?\"\n",
    "pregunta = hub_layer(tf.constant([question]))\n",
    "vec_q = pregunta.numpy()\n",
    "distancias = []\n",
    "respuestas = []\n",
    "\n",
    "for answer,vec_a in zip(sentences,sentence_embeddings.numpy()):\n",
    "    respuestas.append(answer)\n",
    "    distancias.append(np.linalg.norm(vec_q-vec_a))\n",
    "    \n",
    "print(f\"P:{question}\")\n",
    "print(f\"R:{respuestas[np.argmin(distancias)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question  = \"¿Qué ha pasado en Dubrovnik?\"\n",
    "pregunta = hub_layer(tf.constant([question]))\n",
    "vec_q = pregunta.numpy()\n",
    "distancias = []\n",
    "respuestas = []\n",
    "\n",
    "for answer,vec_a in zip(sentences,sentence_embeddings.numpy()):\n",
    "    respuestas.append(answer)\n",
    "    distancias.append(np.linalg.norm(vec_q-vec_a))\n",
    "\n",
    "print(f\"P:{question}\")\n",
    "print(f\"R:{respuestas[np.argmin(distancias)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
