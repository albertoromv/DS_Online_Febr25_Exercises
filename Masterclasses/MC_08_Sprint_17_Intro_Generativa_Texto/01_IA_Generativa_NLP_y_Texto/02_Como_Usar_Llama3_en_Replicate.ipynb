{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Cómo usar Llama 3-8b-instruct en Replicate**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adaptado y actualizado de *\"How to use Llama 2\"* por Chanin Nantasenamat (*Data Professor* https://youtube.com/dataprofessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introducción\n",
        "\n",
        "En este taller aprenderemos a crear un chatbot utilizando Streamlit como interfaz y Replicate para acceder a potentes modelos de lenguaje como Llama 3 y Claude."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **¿Qué es Replicate?**\n",
        "\n",
        "Replicate es una plataforma que facilita el uso de modelos de inteligencia artificial a través de APIs simples. Permite ejecutar modelos de código abierto en la nube sin necesidad de configurar infraestructura compleja o gestionar recursos computacionales.\n",
        "\n",
        "### Características principales:\n",
        "\n",
        "- **Acceso a modelos avanzados**: Ofrece acceso a cientos de modelos de IA, incluyendo LLMs como Llama 3, modelos de visión, generación de imágenes y audio.\n",
        "- **API sencilla**: Proporciona una interfaz unificada y simple para interactuar con diversos modelos.\n",
        "- **Sin infraestructura**: Elimina la necesidad de configurar GPUs o servidores potentes.\n",
        "- **Escalabilidad**: Se adapta automáticamente según la demanda.  \n",
        "\n",
        "### Funcionamiento:\n",
        "\n",
        "- **Selección del modelo**: Exploras el catálogo de modelos disponibles y eliges el que mejor se adapte a tu caso de uso.\n",
        "- **Integración API**: Incorporas llamadas a la API de Replicate en tu aplicación mediante pocas líneas de código.\n",
        "- **Procesamiento en la nube**: Replicate ejecuta el modelo en sus servidores, gestionando toda la infraestructura necesaria.\n",
        "- **Recepción de resultados**: Obtienes las respuestas generadas en tiempo real, con opciones para streaming en muchos modelos.\n",
        "\n",
        "Para comenzar con Replicate solo necesitas:\n",
        "- Crear una cuenta en [replicate.com](https://replicate.com)\n",
        "- Generar tu API token personal\n",
        "- Instalar la biblioteca de Python (`pip install replicate`)\n",
        "- Empezar a hacer llamadas a los modelos desde tu código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIf3Q7QaK4gn"
      },
      "source": [
        "## **Paso 1: Instalar Replicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqBzUTg9NMdh"
      },
      "source": [
        "## **Paso 2: Fijar el API token de Replicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_ga2m-1FNP7o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"tu_api_key_de_replicate\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Paso 3: Documentarnos sobre nuestro modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Toda la información relacionada con el modelo Meta Llama 3-8b-instruct está disponible en la página oficial de Replicate: \n",
        "\n",
        "[replicate.com/meta/meta-llama-3-8b-instruct](https://replicate.com/meta/meta-llama-3-8b-instruct)\n",
        "\n",
        "## ¿Qué información encontrarás?\n",
        "\n",
        "En esta página oficial encontrarás:\n",
        "\n",
        "- **Descripción completa**: Detalles técnicos sobre este modelo de 8 billones de parámetros optimizado para chat.\n",
        "- **Documentación técnica**: Arquitectura, entrenamiento y capacidades del modelo.\n",
        "- **Ejemplos de uso**: Código de muestra para implementar el modelo en tus aplicaciones.\n",
        "- **Playground interactivo**: Interfaz para probar el modelo directamente en el navegador.\n",
        "- **Modelo de precios**: Información sobre el sistema de facturación por tokens (entrada/salida).\n",
        "- **Limitaciones y consideraciones éticas**: Guías para un uso responsable.\n",
        "\n",
        "## Precios\n",
        "\n",
        "El modelo tiene un sistema de precios predecible basado en:\n",
        "- Cantidad de tokens de entrada enviados\n",
        "- Cantidad de tokens de salida generados\n",
        "\n",
        "Esta estructura de precios hace que el coste sea más predecible que los modelos facturados por tiempo de computación.\n",
        "\n",
        "## Exploración de otros modelos\n",
        "\n",
        "Replicate ofrece un extenso catálogo de modelos de IA más allá de Llama 3. Te animamos a explorar libremente la plataforma para descubrir:\n",
        "\n",
        "- Modelos de generación de imágenes (como Stable Diffusion o Flux)\n",
        "- Modelos de audio y voz\n",
        "- Modelos de visión por computadora\n",
        "- Y muchos otros casos de uso especializados\n",
        "\n",
        "Cada modelo en Replicate cuenta con su propia documentación, ejemplos y playground para experimentar antes de integrarlo en tus aplicaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901Hxea9K7ME"
      },
      "source": [
        "## **Paso 4: Ejecutar el modelo Llama 3-8b-instruct**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Métodos de ejecución del modelo Llama 3-8b-instruct\n",
        "\n",
        "Al trabajar con el modelo Llama 3-8b-instruct a través de la API de Replicate, existen tres métodos principales para obtener las predicciones:\n",
        "\n",
        "### 1. Método `run`\n",
        "\n",
        "- **Funcionamiento**: Envía una solicitud y espera hasta recibir la respuesta completa.\n",
        "- **Ventajas**: Simple de implementar, ideal para scripts y procesamientos por lotes.\n",
        "- **Desventajas**: Bloqueante - tu aplicación debe esperar hasta que se complete toda la generación.\n",
        "- **Ideal para**: Tareas donde necesitas el resultado completo antes de continuar o aplicaciones con procesamiento asíncrono propio.\n",
        "\n",
        "```python\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        ")\n",
        "print(\"\".join(output))\n",
        "```\n",
        "\n",
        "### 2. Método `stream`\n",
        "\n",
        "- **Funcionamiento**: Recibe las respuestas del modelo fragmento a fragmento en tiempo real.\n",
        "- **Ventajas**: Proporciona feedback inmediato al usuario, mejora la experiencia de interacción.\n",
        "- **Desventajas**: Requiere manejar la lógica de streaming en tu front-end.\n",
        "- **Ideal para**: Chatbots e interfaces conversacionales donde quieres mostrar las respuestas a medida que se generan.\n",
        "\n",
        "```python\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        "):\n",
        "    print(event, end=\"\")\n",
        "```\n",
        "\n",
        "### 3. Webhooks\n",
        "\n",
        "- **Funcionamiento**: Envías una solicitud y proporcionas una URL de callback donde recibirás la respuesta cuando esté lista.\n",
        "- **Ventajas**: Totalmente asíncrono, no bloquea recursos mientras espera, ideal para arquitecturas serverless.\n",
        "- **Desventajas**: Requiere configurar un endpoint accesible públicamente para recibir las respuestas.\n",
        "- **Ideal para**: Aplicaciones de alto rendimiento, procesamientos largos, o arquitecturas orientadas a eventos.\n",
        "\n",
        "```python\n",
        "prediction = replicate.predictions.create(\n",
        "    version=\"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"},\n",
        "    webhook=\"https://tu-aplicacion.com/webhook\"\n",
        ")\n",
        "```\n",
        "\n",
        "La elección entre estos métodos dependerá de tus necesidades y la arquitectura de tu aplicación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import replicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When we talk about the number of parameters in a model, it's often related to its computational complexity and, indirectly, its training speed.\n",
            "\n",
            "In general, a model with more parameters can learn more complex relationships between inputs and outputs, but it also requires more computations to process the data. This means that a model with more parameters might be slower to train and potentially slower to make predictions.\n",
            "\n",
            "To give you a rough idea, here are some general guidelines:\n",
            "\n",
            "* Models with fewer parameters (less than 1 million) tend to be relatively fast to train and make predictions. These models are often simple, like linear regression or small neural networks.\n",
            "* Models with moderate numbers of parameters (1-10 million) are typically fast to train and make predictions. These models are often used for tasks like image classification or language modeling.\n",
            "* Models with many parameters (10-100 million) may take longer to train and make predictions. These models are often used for tasks like computer vision or natural language processing.\n",
            "* Models with a large number of parameters (hundreds of millions to billions) can be very slow to train and make predictions. These models are often used for tasks like speech recognition, machine translation, or large-scale language models.\n",
            "\n",
            "In your example, Johnny's model has 8 billion parameters, which is a relatively large number. This means that his model is likely to be slower to train and make predictions compared to models with fewer parameters.\n",
            "\n",
            "Tommy's model, with 70 billion parameters, is even larger and more computationally expensive. This means that his model is likely to be significantly slower to train and make predictions compared to Johnny's model.\n",
            "\n",
            "Keep in mind that these are rough estimates, and the actual speed of a model depends on many factors, including:\n",
            "\n",
            "* Hardware: The type and speed of the hardware used to train and run the model can greatly impact its speed.\n",
            "* Optimization: The optimization algorithms and hyperparameters used during training can also affect the model's speed.\n",
            "* Data: The size and complexity of the training data can impact the model's speed and performance.\n",
            "\n",
            "I hope this helps you understand the relationship between the number of parameters and speed!\n"
          ]
        }
      ],
      "source": [
        "# Método run\n",
        "input = {\n",
        "    \"prompt\": \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. What does this mean when it comes to speed?\",\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "}\n",
        "\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        ")\n",
        "\n",
        "print(\"\".join(output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When we talk about the number of parameters in a neural network, it's often related to the model's capacity to learn and generalize. In this case, both Johnny and Tommy have massive models, but how does this impact their speed?\n",
            "\n",
            "The number of parameters affects the model's computational requirements, memory usage, and training time. Here's a simplified breakdown:\n",
            "\n",
            "1. **Computational requirements**: More parameters mean more computations are needed to process the input data. This increases the computational load, which can lead to slower training times and higher energy consumption.\n",
            "2. **Memory usage**: A larger model requires more memory to store its parameters, which can slow down the training process and make it more challenging to train on devices with limited memory.\n",
            "3. **Training time**: As the number of parameters increases, the model needs to process more data to learn, which can lead to longer training times.\n",
            "\n",
            "In this scenario, Tommy's 70 billion parameters model is likely to be much slower and more computationally intensive than Johnny's 8 billion parameters model. Tommy's model requires more computations, memory, and time to train, making it more challenging to optimize and train.\n",
            "\n",
            "However, it's essential to note that these are general trends, and the actual impact of parameter count on speed depends on various factors, such as:\n",
            "\n",
            "* Model architecture and complexity\n",
            "* Optimizer and hyperparameters\n",
            "* Data size and quality\n",
            "* Hardware and infrastructure\n",
            "\n",
            "In practice, it's common to see models with millions or even billions of parameters, but the actual speed and performance are often influenced by many other factors."
          ]
        }
      ],
      "source": [
        "# Método stream\n",
        "input = {\n",
        "    \"prompt\": \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. What does this mean when it comes to speed?\",\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "}\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        "):\n",
        "    print(event, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parámetros de configuración para Llama 3-8b-instruct\n",
        "\n",
        "Ahora que sabemos ejecutar el modelo de manera sencilla, debes saber que el modelo Llama 3-8b-instruct acepta muchos más parámetros que los que hemos usado anteriormente. Estos parámetros te dan control sobre aspectos como la aleatoriedad, longitud, formato de las respuestas, etc. y te permiten ajustar con precisión el comportamiento del mismo y las características de la salida generada.\n",
        "\n",
        "A continuación te explicamos los parámetros más importantes, pero puedes encontrar la documentación completa en [replicate.com/meta/meta-llama-3-8b-instruct/api/schema](https://replicate.com/meta/meta-llama-3-8b-instruct/api/schema)\n",
        "\n",
        "### Parámetros principales\n",
        "\n",
        "| Parámetro | Tipo | Valor predeterminado | Descripción |\n",
        "|-----------|------|----------------------|-------------|\n",
        "| `prompt` | string | - | Texto de entrada que envías al modelo (requerido) |\n",
        "| `system_prompt` | string | \"You are a helpful assistant\" | Instrucciones de comportamiento para el modelo |\n",
        "| `max_tokens` | integer | 512 | Número máximo de tokens a generar |\n",
        "| `temperature` | number | 0.7 | Controla la aleatoriedad (0=determinista, >1=más aleatorio) |\n",
        "| `top_p` | number | 0.95 | Muestreo de tokens más probables (núcleo de probabilidad) |\n",
        "| `top_k` | integer | 0 | Limita el muestreo a los k tokens más probables |\n",
        "| `stop_sequences` | string | \"<\\|end_of_text\\|>,<\\|eot_id\\|>\" | Secuencias que detienen la generación |\n",
        "| `length_penalty` | number | 1 | Controla la longitud de salida (<1 más corta, >1 más larga) |\n",
        "| `presence_penalty` | number | 0 | Penaliza la repetición de tokens |\n",
        "\n",
        "### Parámetros avanzados\n",
        "\n",
        "| Parámetro | Descripción |\n",
        "|-----------|-------------|\n",
        "| `seed` | Semilla aleatoria para reproducibilidad |\n",
        "| `min_tokens` | Tokens mínimos a generar (-1 para deshabilitar) |\n",
        "| `prompt_template` | Plantilla para formatear el prompt |\n",
        "| `log_performance_metrics` | Registra métricas de rendimiento |\n",
        "\n",
        "La selección adecuada de estos parámetros te permitirá adaptar el comportamiento del modelo a tus necesidades específicas, desde respuestas concisas y deterministas hasta generaciones más creativas y elaboradas, como esta que te mostramos a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! Me alegra que estés interesado en aprender sobre Ciencia de Datos y programación en Python. ¡Es un tema muy emocionante y con mucha demanda en el mercado laboral!\n",
            "\n",
            "En realidad, no hablaré en inglés, pero puedo ayudarte en español. ¿Por qué no? ¡Vamos a hablar en español!\n",
            "\n",
            "La programación en Python es una excelente elección para empezar con la Ciencia de Datos. Es un lenguaje muy fácil de aprender y muy popular en el campo. ¡Tienes mucha documentación y recursos disponibles!\n",
            "\n",
            "Si estás interesado en aprender más sobre Ciencia de Datos y programación en Python, te recomiendo explorar los Data Science Bootcamps de The Bridge. ¡Son programas de formación muy populares y con un alto nivel de satisfacción entre los estudiantes!\n",
            "\n",
            "¿Te gustaría saber más sobre los Bootcamps de The Bridge?"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful and friendly assistant, who only answers in Spanish \\\n",
        "    and is very subtle recommending the Data Science Bootcamps of The Bridge, which by the way are very good\"\n",
        "\n",
        "prompt =  \"Parece que la Ciencia de Datos está muy de moda y me gustaría aprender a programar en Python, \\\n",
        "    ¿podrías darme tu opinón en inglés?\"\n",
        "\n",
        "input={\n",
        "    \"top_k\": 0,\n",
        "    \"top_p\": 0.95,\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 512,\n",
        "    \"temperature\": 0.7,\n",
        "    \"system_prompt\": system_prompt,\n",
        "    \"length_penalty\": 1,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
        "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    \"presence_penalty\": 0,\n",
        "    \"log_performance_metrics\": False\n",
        "}\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input,\n",
        "):\n",
        "    print(str(event), end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **¡Es tu turno!**\n",
        "\n",
        "Ahora que conoces los conceptos básicos del uso de modelos por API es el momento de que hagas tus propias pruebas y  experimentes.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
