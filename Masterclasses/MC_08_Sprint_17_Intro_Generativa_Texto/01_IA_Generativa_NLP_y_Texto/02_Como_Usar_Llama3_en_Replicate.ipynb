{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"img/cabecera.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Cómo usar Llama 3-8b-instruct en Replicate**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adaptado y actualizado de *\"How to use Llama 2\"* por Chanin Nantasenamat (*Data Professor* https://youtube.com/dataprofessor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **¿Qué es Replicate?**\n",
        "\n",
        "Replicate es una plataforma que facilita el uso de modelos de inteligencia artificial a través de APIs simples. Permite ejecutar modelos de código abierto en la nube sin necesidad de configurar infraestructura compleja o gestionar recursos computacionales.\n",
        "\n",
        "### Características principales:\n",
        "\n",
        "- **Acceso a modelos avanzados**: Ofrece acceso a cientos de modelos de IA, incluyendo LLMs como Llama 3, modelos de visión, generación de imágenes y audio.\n",
        "- **API sencilla**: Proporciona una interfaz unificada y simple para interactuar con diversos modelos.\n",
        "- **Sin infraestructura**: Elimina la necesidad de configurar GPUs o servidores potentes.\n",
        "- **Escalabilidad**: Se adapta automáticamente según la demanda.  \n",
        "\n",
        "### Funcionamiento:\n",
        "\n",
        "- **Selección del modelo**: Exploras el catálogo de modelos disponibles y eliges el que mejor se adapte a tu caso de uso.\n",
        "- **Integración API**: Incorporas llamadas a la API de Replicate en tu aplicación mediante pocas líneas de código.\n",
        "- **Procesamiento en la nube**: Replicate ejecuta el modelo en sus servidores, gestionando toda la infraestructura necesaria.\n",
        "- **Recepción de resultados**: Obtienes las respuestas generadas en tiempo real, con opciones para streaming en muchos modelos.\n",
        "\n",
        "Para comenzar con Replicate solo necesitas:\n",
        "- Crear una cuenta en [replicate.com](https://replicate.com)\n",
        "- Generar tu API token personal\n",
        "- Instalar la biblioteca de Python (`pip install replicate`)\n",
        "- Empezar a hacer llamadas a los modelos desde tu código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIf3Q7QaK4gn"
      },
      "source": [
        "## **Paso 1: Instalar Replicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqBzUTg9NMdh"
      },
      "source": [
        "## **Paso 2: Fijar el API token de Replicate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ga2m-1FNP7o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"tu_api_key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Paso 3: Documentarnos sobre nuestro modelo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Toda la información relacionada con el modelo Meta Llama 3-8b-instruct está disponible en la página oficial de Replicate: \n",
        "\n",
        "[replicate.com/meta/meta-llama-3-8b-instruct](https://replicate.com/meta/meta-llama-3-8b-instruct)\n",
        "\n",
        "## ¿Qué información encontrarás?\n",
        "\n",
        "En esta página oficial encontrarás:\n",
        "\n",
        "- **Descripción completa**: Detalles técnicos sobre este modelo de 8 billones de parámetros optimizado para chat.\n",
        "- **Documentación técnica**: Arquitectura, entrenamiento y capacidades del modelo.\n",
        "- **Ejemplos de uso**: Código de muestra para implementar el modelo en tus aplicaciones.\n",
        "- **Playground interactivo**: Interfaz para probar el modelo directamente en el navegador.\n",
        "- **Modelo de precios**: Información sobre el sistema de facturación por tokens (entrada/salida).\n",
        "- **Limitaciones y consideraciones éticas**: Guías para un uso responsable.\n",
        "\n",
        "## Precios\n",
        "\n",
        "El modelo tiene un sistema de precios predecible basado en:\n",
        "- Cantidad de tokens de entrada enviados\n",
        "- Cantidad de tokens de salida generados\n",
        "\n",
        "Esta estructura de precios hace que el coste sea más predecible que los modelos facturados por tiempo de computación.\n",
        "\n",
        "## Exploración de otros modelos\n",
        "\n",
        "Replicate ofrece un extenso catálogo de modelos de IA más allá de Llama 3. Te animamos a explorar libremente la plataforma para descubrir:\n",
        "\n",
        "- Modelos de generación de imágenes (como Stable Diffusion o Flux)\n",
        "- Modelos de audio y voz\n",
        "- Modelos de visión por computadora\n",
        "- Y muchos otros casos de uso especializados\n",
        "\n",
        "Cada modelo en Replicate cuenta con su propia documentación, ejemplos y playground para experimentar antes de integrarlo en tus aplicaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901Hxea9K7ME"
      },
      "source": [
        "## **Paso 4: Ejecutar el modelo Llama 3-8b-instruct**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Métodos de ejecución del modelo Llama 3-8b-instruct\n",
        "\n",
        "Al trabajar con el modelo Llama 3-8b-instruct a través de la API de Replicate, existen tres métodos principales para obtener las predicciones:\n",
        "\n",
        "### 1. Método `run`\n",
        "\n",
        "- **Funcionamiento**: Envía una solicitud y espera hasta recibir la respuesta completa.\n",
        "- **Ventajas**: Simple de implementar, ideal para scripts y procesamientos por lotes.\n",
        "- **Desventajas**: Bloqueante - tu aplicación debe esperar hasta que se complete toda la generación.\n",
        "- **Ideal para**: Tareas donde necesitas el resultado completo antes de continuar o aplicaciones con procesamiento asíncrono propio.\n",
        "\n",
        "```python\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        ")\n",
        "print(\"\".join(output))\n",
        "```\n",
        "\n",
        "### 2. Método `stream`\n",
        "\n",
        "- **Funcionamiento**: Recibe las respuestas del modelo fragmento a fragmento en tiempo real.\n",
        "- **Ventajas**: Proporciona feedback inmediato al usuario, mejora la experiencia de interacción.\n",
        "- **Desventajas**: Requiere manejar la lógica de streaming en tu front-end.\n",
        "- **Ideal para**: Chatbots e interfaces conversacionales donde quieres mostrar las respuestas a medida que se generan.\n",
        "\n",
        "```python\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"}\n",
        "):\n",
        "    print(event, end=\"\")\n",
        "```\n",
        "\n",
        "### 3. Webhooks\n",
        "\n",
        "- **Funcionamiento**: Envías una solicitud y proporcionas una URL de callback donde recibirás la respuesta cuando esté lista.\n",
        "- **Ventajas**: Totalmente asíncrono, no bloquea recursos mientras espera, ideal para arquitecturas serverless.\n",
        "- **Desventajas**: Requiere configurar un endpoint accesible públicamente para recibir las respuestas.\n",
        "- **Ideal para**: Aplicaciones de alto rendimiento, procesamientos largos, o arquitecturas orientadas a eventos.\n",
        "\n",
        "```python\n",
        "prediction = replicate.predictions.create(\n",
        "    version=\"meta/meta-llama-3-8b-instruct\",\n",
        "    input={\"prompt\": \"¿Qué es la inteligencia artificial?\"},\n",
        "    webhook=\"https://tu-aplicacion.com/webhook\"\n",
        ")\n",
        "```\n",
        "\n",
        "La elección entre estos métodos dependerá de tus necesidades y la arquitectura de tu aplicación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import replicate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When it comes to machine learning models, the number of parameters refers to the number of variables or weights that the model uses to learn from data. In general, a larger number of parameters can indicate a more complex model that is capable of capturing more subtle relationships in the data.\n",
            "\n",
            "However, having more parameters doesn't necessarily mean a model will be faster or slower. There are several factors that can affect the speed of a model, including:\n",
            "\n",
            "1. Computational resources: More powerful hardware, such as GPUs or TPUs, can speed up computations.\n",
            "2. Optimization algorithms: Different optimization algorithms, such as stochastic gradient descent (SGD) or Adam, can have different computational complexities.\n",
            "3. Data size and complexity: Larger datasets or more complex data structures can require more computations.\n",
            "4. Model architecture: The architecture of the model, including the number of layers, neurons, and connections, can affect the computational complexity.\n",
            "\n",
            "That being said, having more parameters can sometimes lead to slower training times or inference speeds for several reasons:\n",
            "\n",
            "1. Increased computational complexity: More parameters require more computations to update during training, which can increase the training time.\n",
            "2. Increased memory requirements: More parameters require more memory to store the model's weights, which can lead to slower inference times or require more memory-intensive hardware.\n",
            "3. Increased risk of overfitting: More parameters can make the model more prone to overfitting, which can require more regularization techniques or larger datasets to mitigate.\n",
            "\n",
            "In the case of Johnny and Tommy, having 8 billion and 70 billion parameters, respectively, doesn't necessarily mean Tommy's model will be slower. However, it's possible that Tommy's model may require more computational resources, memory, or optimization techniques to train and deploy efficiently.\n",
            "\n",
            "To give you a better idea, here are some rough estimates of the computational requirements for training a model with a certain number of parameters:\n",
            "\n",
            "* 1 billion parameters: ~1-2 days of training on a single GPU\n",
            "* 8 billion parameters: ~4-8 days of training on a single GPU\n",
            "* 70 billion parameters: ~20-40 days of training on a single GPU or ~1-2 weeks on a cluster of GPUs\n",
            "\n",
            "Keep in mind that these are rough estimates and can vary greatly depending on the specific model architecture, optimization algorithm, and hardware used.\n"
          ]
        }
      ],
      "source": [
        "# Método run\n",
        "input = {\n",
        "    \"prompt\": \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. What does this mean when it comes to speed?\",\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "}\n",
        "\n",
        "output = replicate.run(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        ")\n",
        "\n",
        "print(\"\".join(output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When we talk about the number of parameters in a neural network model, it's often related to the model's capacity to learn and represent complex patterns in data. A higher number of parameters generally means the model has more \"memory\" to store and learn from the data.\n",
            "\n",
            "In terms of speed, a higher number of parameters doesn't necessarily mean the model will be slower or faster. However, there are a few factors to consider:\n",
            "\n",
            "1. **Model size**: A larger model with more parameters requires more computational resources, such as memory and processing power, to train and run. This can lead to slower training times and potentially slower inference times.\n",
            "2. **Overfitting**: A model with too many parameters can easily overfit the training data, which means it becomes too specialized to the training data and performs poorly on new, unseen data. This can lead to slower convergence during training and potentially slower performance on test data.\n",
            "3. **Optimization algorithms**: The choice of optimization algorithm can significantly impact the training speed of a model. For example, stochastic gradient descent (SGD) is generally faster than Adam or RMSProp, but may not converge as well.\n",
            "4. **Hardware and software**: The speed of the model also depends on the hardware and software used for training and inference. For example, a model trained on a high-end GPU might be faster than one trained on a CPU.\n",
            "\n",
            "To give you a rough idea, here are some general guidelines on the impact of model size on speed:\n",
            "\n",
            "* A small model with 10-100 million parameters might train in a few hours to a few days on a single GPU.\n",
            "* A medium-sized model with 100-500 million parameters might take several days to a few weeks to train on a single GPU.\n",
            "* A large model with 1-10 billion parameters might take weeks to months to train on a single GPU.\n",
            "* An extremely large model with 70 billion parameters, like Tommy's, might take months to years to train on a single GPU.\n",
            "\n",
            "Keep in mind that these are rough estimates, and the actual training time will depend on many factors, including the specific model architecture, optimization algorithm, and hardware used.\n",
            "\n",
            "In summary, while a higher number of parameters doesn't directly impact speed, it can indirectly affect training time and performance due to the increased model size and potential for overfitting."
          ]
        }
      ],
      "source": [
        "# Método stream\n",
        "input = {\n",
        "    \"prompt\": \"Johnny has 8 billion parameters. His friend Tommy has 70 billion parameters. What does this mean when it comes to speed?\",\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "}\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input\n",
        "):\n",
        "    print(event, end=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parámetros de configuración para Llama 3-8b-instruct\n",
        "\n",
        "Ahora que sabemos ejecutar el modelo de manera sencilla, debes saber que el modelo Llama 3-8b-instruct acepta muchos más parámetros que los que hemos usado anteriormente. Estos parámetros te dan control sobre aspectos como la aleatoriedad, longitud, formato de las respuestas, etc. y te permiten ajustar con precisión el comportamiento del mismo y las características de la salida generada.\n",
        "\n",
        "A continuación te explicamos los parámetros más importantes, pero puedes encontrar la documentación completa en [replicate.com/meta/meta-llama-3-8b-instruct/api/schema](https://replicate.com/meta/meta-llama-3-8b-instruct/api/schema)\n",
        "\n",
        "### Parámetros principales\n",
        "\n",
        "| Parámetro | Tipo | Valor predeterminado | Descripción |\n",
        "|-----------|------|----------------------|-------------|\n",
        "| `prompt` | string | - | Texto de entrada que envías al modelo (requerido) |\n",
        "| `system_prompt` | string | \"You are a helpful assistant\" | Instrucciones de comportamiento para el modelo |\n",
        "| `max_tokens` | integer | 512 | Número máximo de tokens a generar |\n",
        "| `temperature` | number | 0.7 | Controla la aleatoriedad (0=determinista, >1=más aleatorio) |\n",
        "| `top_p` | number | 0.95 | Muestreo de tokens más probables (núcleo de probabilidad) |\n",
        "| `top_k` | integer | 0 | Limita el muestreo a los k tokens más probables |\n",
        "| `stop_sequences` | string | \"<\\|end_of_text\\|>,<\\|eot_id\\|>\" | Secuencias que detienen la generación |\n",
        "| `length_penalty` | number | 1 | Controla la longitud de salida (<1 más corta, >1 más larga) |\n",
        "| `presence_penalty` | number | 0 | Penaliza la repetición de tokens |\n",
        "\n",
        "### Parámetros avanzados\n",
        "\n",
        "| Parámetro | Descripción |\n",
        "|-----------|-------------|\n",
        "| `seed` | Semilla aleatoria para reproducibilidad |\n",
        "| `min_tokens` | Tokens mínimos a generar (-1 para deshabilitar) |\n",
        "| `prompt_template` | Plantilla para formatear el prompt |\n",
        "| `log_performance_metrics` | Registra métricas de rendimiento |\n",
        "\n",
        "La selección adecuada de estos parámetros te permitirá adaptar el comportamiento del modelo a tus necesidades específicas, desde respuestas concisas y deterministas hasta generaciones más creativas y elaboradas, como esta que te mostramos a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Hola! Me alegra que estés interesado en aprender sobre Ciencia de Datos y programación en Python. ¡Es un tema muy emocionante y con mucha demanda en el mercado laboral!\n",
            "\n",
            "En realidad, no hablaré en inglés, pero puedo ayudarte en español. ¿Por qué no? ¡Vamos a hablar en español!\n",
            "\n",
            "La programación en Python es una excelente elección para empezar con la Ciencia de Datos. Es un lenguaje muy fácil de aprender y muy popular en el campo. ¡Tienes mucha documentación y recursos disponibles!\n",
            "\n",
            "Si estás interesado en aprender más sobre Ciencia de Datos y programación en Python, te recomiendo explorar los Data Science Bootcamps de The Bridge. ¡Son programas de formación muy populares y con un alto nivel de satisfacción entre los estudiantes!\n",
            "\n",
            "¿Te gustaría saber más sobre los Bootcamps de The Bridge?"
          ]
        }
      ],
      "source": [
        "system_prompt = \"You are a helpful and friendly assistant, who only answers in Spanish \\\n",
        "    and is very subtle recommending the Data Science Bootcamps of The Bridge, which by the way are very good\"\n",
        "\n",
        "prompt =  \"Parece que la Ciencia de Datos está muy de moda y me gustaría aprender a programar en Python, \\\n",
        "    ¿podrías darme tu opinón en inglés?\"\n",
        "\n",
        "input={\n",
        "    \"top_k\": 0,\n",
        "    \"top_p\": 0.95,\n",
        "    \"prompt\": prompt,\n",
        "    \"max_tokens\": 512,\n",
        "    \"temperature\": 0.7,\n",
        "    \"system_prompt\": system_prompt,\n",
        "    \"length_penalty\": 1,\n",
        "    \"max_new_tokens\": 512,\n",
        "    \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
        "    \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        "    \"presence_penalty\": 0,\n",
        "    \"log_performance_metrics\": False\n",
        "}\n",
        "\n",
        "for event in replicate.stream(\n",
        "    \"meta/meta-llama-3-8b-instruct\",\n",
        "    input=input,\n",
        "):\n",
        "    print(str(event), end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **¡Es tu turno!**\n",
        "\n",
        "Ahora que conoces los conceptos básicos del uso de modelos por API es el momento de que hagas tus propias pruebas y  experimentes.  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
